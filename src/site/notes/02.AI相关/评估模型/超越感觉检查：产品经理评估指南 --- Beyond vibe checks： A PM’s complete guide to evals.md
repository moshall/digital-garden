---
{"dg-publish":true,"permalink":"/02.AIç›¸å…³/è¯„ä¼°æ¨¡å‹/è¶…è¶Šæ„Ÿè§‰æ£€æŸ¥ï¼šäº§å“ç»ç†è¯„ä¼°æŒ‡å— --- Beyond vibe checksï¼š A PMâ€™s complete guide to evals/","title":"è¶…è¶Šæ„Ÿè§‰æ£€æŸ¥äº§å“ç»ç†è¯„ä¼°æŒ‡å— --- Beyond vibe checks:A PMâ€™s complete guide to evals"}
---


Writing evals is quickly becoming a core skill for anyone building AI products (which will soon be everyone). Yet thereâ€™s very little specific advice on how to get good at it. Below youâ€™ll find everything you need to understand wtf evals are, why they are so important, and how to master this emerging skill.  
æ’°å†™è¯„ä¼°æŠ¥å‘Šæ­£è¿…é€Ÿæˆä¸ºä»»ä½•æ„å»º AI äº§å“ï¼ˆå¾ˆå¿«å°†æ¶‰åŠæ‰€æœ‰äººï¼‰çš„æ ¸å¿ƒæŠ€èƒ½ã€‚ç„¶è€Œï¼Œå…³äºå¦‚ä½•æ“…é•¿è¿™é¡¹æŠ€èƒ½çš„å…·ä½“å»ºè®®å´éå¸¸å°‘ã€‚ä»¥ä¸‹æ˜¯ä½ éœ€è¦äº†è§£çš„ä¸€åˆ‡ï¼ŒåŒ…æ‹¬è¯„ä¼°æŠ¥å‘Šæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆå®ƒä»¬å¦‚æ­¤é‡è¦ï¼Œä»¥åŠå¦‚ä½•æŒæ¡è¿™é¡¹æ–°å…´æŠ€èƒ½ã€‚

Aman Khan runs a popular [course on evals developed with Andrew Ng](https://www.deeplearning.ai/short-courses/evaluating-ai-agents/), is Director of Product at Arize AI (a leading AI company), and has been a product leader at Spotify, Cruise, Zipline, and Apple. He was also a [past podcast guest](https://www.youtube.com/watch?v=E_rNotqs--I) and is launching his first [Maven course on AI product management](https://maven.com/aman-khan/thriving-as-an-ai-pm) this spring. If youâ€™re looking to get more hands-on, definitely check out Amanâ€™s upcoming free 30-minute lightning lesson on April 18th: [Mastering Evals as an AI Product Manager](https://maven.com/p/e55ece/mastering-evals-as-an-ai-product-manager?utm_campaign=MzgyNTQ0&utm_medium=ll_share_link&utm_source=instructor). You can find Aman on [X](https://x.com/_amankhan), [LinkedIn](https://www.linkedin.com/in/amanberkeley/), and [Substack](http://aiproductplaybook.com/).  
é˜¿æ›¼Â·æ±—å¼€è®¾äº†ä¸€é—¨ä¸ Andrew Ng å…±åŒå¼€å‘çš„å…³äºè¯„ä¼°çš„æµè¡Œè¯¾ç¨‹ï¼Œæ˜¯ Arize AIï¼ˆä¸€å®¶é¢†å…ˆçš„ AI å…¬å¸ï¼‰çš„äº§å“æ€»ç›‘ï¼Œæ›¾åœ¨ Spotifyã€Cruiseã€Zipline å’Œè‹¹æœå…¬å¸æ‹…ä»»äº§å“é¢†å¯¼ã€‚ä»–è¿˜æ›¾æ˜¯æ’­å®¢çš„å˜‰å®¾ï¼Œå¹¶å°†åœ¨ä»Šå¹´æ˜¥å¤©æ¨å‡ºä»–çš„ç¬¬ä¸€é—¨å…³äº AI äº§å“ç®¡ç†çš„ Maven è¯¾ç¨‹ã€‚å¦‚æœä½ æƒ³è¦æ›´æ·±å…¥åœ°äº†è§£ï¼Œç»å¯¹è¦çœ‹çœ‹é˜¿æ›¼å³å°†åœ¨ 4 æœˆ 18 æ—¥æ¨å‡ºçš„å…è´¹ 30 åˆ†é’Ÿé—ªç”µè¯¾ç¨‹ï¼šä½œä¸º AI äº§å“ç»ç†æŒæ¡è¯„ä¼°ã€‚ä½ å¯ä»¥åœ¨ Xã€LinkedIn å’Œ Substack ä¸Šæ‰¾åˆ°é˜¿æ›¼ã€‚

Now, on to the post. . .  
ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¿›å…¥æ­£æ–‡â€¦â€¦



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164841060.jpeg?cd-oss-obs)





After years of building AI products, Iâ€™ve noticed something surprising: every PM building with generative AI obsesses over crafting better prompts and using the latest LLM, yet almost no one masters the hidden lever behind every exceptional AI product: evaluations. Evals are the only way you can break down each step in the system and measure _specifically_ what impact an individual change might have on a product, giving you the data and confidence to take the right next step. Prompts may make headlines, but evals quietly decide whether your product thrives or dies. In fact, Iâ€™d argue that the ability to write great evals isnâ€™t just importantâ€”itâ€™s rapidly becoming the defining skill for AI PMs in 2025 and beyond.  
ç»è¿‡å¤šå¹´æ„å»º AI äº§å“ï¼Œæˆ‘å‘ç°äº†ä¸€ä¸ªæƒŠäººçš„ç°è±¡ï¼šæ¯ä¸ªä½¿ç”¨ç”Ÿæˆå¼ AI æ„å»ºäº§å“çš„äº§å“ç»ç†éƒ½ç—´è¿·äºåˆ¶ä½œæ›´å¥½çš„æç¤ºè¯å’Œä½¿ç”¨æœ€æ–°çš„LLMï¼Œç„¶è€Œï¼Œå‡ ä¹æ²¡æœ‰äººæŒæ¡æ¯ä¸ªå“è¶Š AI äº§å“èƒŒåçš„éšè—æ æ†ï¼šè¯„ä¼°ã€‚è¯„ä¼°æ˜¯å”¯ä¸€èƒ½å¤Ÿåˆ†è§£ç³»ç»Ÿæ¯ä¸€æ­¥å¹¶å…·ä½“è¡¡é‡å•ä¸ªå˜åŒ–å¯èƒ½å¯¹äº§å“äº§ç”Ÿçš„å½±å“çš„æ–¹æ³•ï¼Œä¸ºä½ æä¾›æ•°æ®å’Œä¿¡å¿ƒï¼Œè®©ä½ èƒ½å¤Ÿé‡‡å–æ­£ç¡®çš„ä¸‹ä¸€æ­¥ã€‚æç¤ºè¯å¯èƒ½ä¼šæˆä¸ºå¤´æ¡æ–°é—»ï¼Œä½†è¯„ä¼°å´é»˜é»˜åœ°å†³å®šäº†ä½ çš„äº§å“æ˜¯ç¹è£è¿˜æ˜¯æ­»äº¡ã€‚äº‹å®ä¸Šï¼Œæˆ‘è®¤ä¸ºç¼–å†™å‡ºè‰²è¯„ä¼°çš„èƒ½åŠ›ä¸ä»…å¾ˆé‡è¦â€”â€”å®ƒæ­£åœ¨è¿…é€Ÿæˆä¸º 2025 å¹´åŠä»¥å AI äº§å“ç»ç†çš„æ ‡å¿—æ€§æŠ€èƒ½ã€‚



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164842818.jpeg?cd-oss-obs)



If youâ€™re not actively building this muscle, youâ€™re likely missing your biggest opportunity for impact-building AI products.  
å¦‚æœä½ ä¸ç§¯æé”»ç‚¼è¿™å—è‚Œè‚‰ï¼Œä½ å¯èƒ½ä¼šé”™è¿‡æ„å»ºå½±å“æœ€å¤§çš„ AI äº§å“çš„æœ€å¤§æœºä¼šã€‚

Let me show you why.  
è®©æˆ‘æ¥å‘Šè¯‰ä½ åŸå› ã€‚

Letâ€™s imagine youâ€™re building a trip-planning AI agent for a travel-booking website. The idea: your users type in natural language requests like â€œ_I want a relaxing weekend getaway near San Francisco for under $1,000_,â€ and the agent goes off to research the best flights, hotels, and local experiences tailored to their preferences.  
è®©æˆ‘ä»¬æƒ³è±¡ä½ æ­£åœ¨ä¸ºä¸€å®¶æ—…è¡Œé¢„è®¢ç½‘ç«™æ„å»ºä¸€ä¸ªè¡Œç¨‹è§„åˆ’äººå·¥æ™ºèƒ½ä»£ç†ã€‚æƒ³æ³•æ˜¯ï¼šä½ çš„ç”¨æˆ·è¾“å…¥è‡ªç„¶è¯­è¨€è¯·æ±‚ï¼Œä¾‹å¦‚â€œæˆ‘æƒ³åœ¨æ—§é‡‘å±±é™„è¿‘èŠ±è´¹ä¸åˆ° 1000 ç¾å…ƒçš„æ”¾æ¾å‘¨æœ«åº¦å‡â€ï¼Œç„¶åä»£ç†å¼€å§‹ç ”ç©¶æœ€é€‚åˆä»–ä»¬åå¥½çš„æœ€ä½³èˆªç­ã€é…’åº—å’Œå½“åœ°ä½“éªŒã€‚

To build this agent, youâ€™d typically start by selecting an LLM (e.g. GPT-4o, Claude, or Gemini) and then design prompts (specific instructions) that guide the LLM to interpret user requests and respond appropriately. Your first impulse might be to feed user questions into the LLM directly to get out responses one by one, as with a simple chatbot, before adding capabilities to turn it into a true â€œagent.â€ When you extend your LLM-plus-prompt by giving it access to external toolsâ€”like flight APIs, hotel databases, or mapping servicesâ€”you allow it to execute tasks, retrieve information, and respond dynamically to user requests. At that point, your simple LLM-plus-prompt evolves into an AI agent, capable of handling complex, multi-step interactions with your users. For internal testing, you might experiment with common scenarios and manually verify that the outputs make sense.  
æ„å»ºè¿™ä¸ªä»£ç†ï¼Œä½ é€šå¸¸ä¼šå…ˆé€‰æ‹©ä¸€ä¸ªLLMï¼ˆä¾‹å¦‚ GPT-4oã€Claude æˆ– Geminiï¼‰ï¼Œç„¶åè®¾è®¡æç¤ºï¼ˆå…·ä½“æŒ‡ä»¤ï¼‰æ¥å¼•å¯¼LLMè§£é‡Šç”¨æˆ·è¯·æ±‚å¹¶é€‚å½“åœ°å“åº”ã€‚ä½ çš„ç¬¬ä¸€ä¸ªå†²åŠ¨å¯èƒ½æ˜¯ç›´æ¥å°†ç”¨æˆ·é—®é¢˜è¾“å…¥åˆ°LLMä¸­ï¼Œé€ä¸ªè·å–å“åº”ï¼Œå°±åƒç®€å•çš„èŠå¤©æœºå™¨äººä¸€æ ·ï¼Œç„¶åå†æ·»åŠ åŠŸèƒ½å°†å…¶å˜æˆçœŸæ­£çš„â€œä»£ç†â€ã€‚å½“ä½ é€šè¿‡æä¾›å¯¹å¤–éƒ¨å·¥å…·çš„è®¿é—®â€”â€”å¦‚èˆªç­ APIã€é…’åº—æ•°æ®åº“æˆ–åœ°å›¾æœåŠ¡â€”â€”æ¥æ‰©å±•ä½ çš„LLM-plus-prompt æ—¶ï¼Œä½ å…è®¸å®ƒæ‰§è¡Œä»»åŠ¡ã€æ£€ç´¢ä¿¡æ¯å¹¶åŠ¨æ€å“åº”ç”¨æˆ·è¯·æ±‚ã€‚åˆ°é‚£æ—¶ï¼Œä½ çš„ç®€å•LLM-plus-prompt å°±æ¼”å˜æˆäº†ä¸€ä¸ª AI ä»£ç†ï¼Œèƒ½å¤Ÿå¤„ç†ä¸ç”¨æˆ·å¤æ‚ã€å¤šæ­¥éª¤çš„äº¤äº’ã€‚å¯¹äºå†…éƒ¨æµ‹è¯•ï¼Œä½ å¯èƒ½ä¼šå¯¹å¸¸è§åœºæ™¯è¿›è¡Œå®éªŒï¼Œå¹¶æ‰‹åŠ¨éªŒè¯è¾“å‡ºæ˜¯å¦åˆç†ã€‚

Everything seems greatâ€”until you launch. Suddenly, frustrated customers flood support because the agent booked them flights to San Diego instead of San Francisco. Yikes. How did this happen? And more importantly, how could you have caught and prevented this error earlier?  
ä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆå®Œç¾â€”â€”ç›´åˆ°ä½ æ¨å‡ºäº§å“ã€‚çªç„¶ï¼Œæ²®ä¸§çš„å®¢æˆ·æ¶Œå…¥æ”¯æŒéƒ¨é—¨ï¼Œå› ä¸ºä»£ç†å•†ä¸ºä»–ä»¬é¢„è®¢äº†åœ£åœ°äºšå“¥çš„èˆªç­è€Œä¸æ˜¯æ—§é‡‘å±±ã€‚å“å‘€ã€‚è¿™æ˜¯æ€ä¹ˆå‘ç”Ÿçš„ï¼Ÿæ›´é‡è¦çš„æ˜¯ï¼Œä½ å¦‚ä½•èƒ½æ›´æ—©åœ°æ•æ‰å¹¶é¢„é˜²è¿™ä¸ªé”™è¯¯ï¼Ÿ

This is where evals come in.  
è¿™æ˜¯è¯„ä¼°ä»‹å…¥çš„åœ°æ–¹ã€‚

Evals are how you measure the quality and effectiveness of your AI system. They act like regression tests or benchmarks, clearly defining what â€œgoodâ€ actually looks like for your AI product beyond the kind of simple latency or pass/fail checks youâ€™d usually use for software.  
è¯„ä¼°æ˜¯è¡¡é‡æ‚¨ AI ç³»ç»Ÿè´¨é‡å’Œæ•ˆæœçš„æ–¹æ³•ã€‚å®ƒä»¬ç±»ä¼¼äºå›å½’æµ‹è¯•æˆ–åŸºå‡†æµ‹è¯•ï¼Œæ˜ç¡®ç•Œå®šäº†æ‚¨çš„ AI äº§å“â€œè‰¯å¥½â€çš„æ ‡å‡†ï¼Œè€Œä¸ä»…ä»…æ˜¯é€šå¸¸ç”¨äºè½¯ä»¶çš„ç®€å•å»¶è¿Ÿæˆ–é€šè¿‡/å¤±è´¥æ£€æŸ¥ã€‚

Evaluating AI systems is less like traditional software testing and more like giving someone a driving test:  
è¯„ä¼°äººå·¥æ™ºèƒ½ç³»ç»Ÿæ›´åƒæ˜¯ç»™æŸäººè¿›è¡Œé©¾é©¶è€ƒè¯•ï¼Œè€Œä¸æ˜¯åƒä¼ ç»Ÿçš„è½¯ä»¶æµ‹è¯•é‚£æ ·

*   **Awareness:** Can it correctly interpret signals and react appropriately to changing conditions?  
    æ„è¯†ï¼šå®ƒèƒ½å¦æ­£ç¡®è§£è¯»ä¿¡å·å¹¶é€‚å½“åœ°å¯¹å˜åŒ–æ¡ä»¶åšå‡ºååº”ï¼Ÿ
    
*   **Decision-making:** Does it reliably make the correct choices, even in unpredictable situations?  
    å†³ç­–ï¼šå®ƒæ˜¯å¦åœ¨ä¸å¯é¢„æµ‹çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å¯é åœ°åšå‡ºæ­£ç¡®çš„é€‰æ‹©ï¼Ÿ
    
*   **Safety:** Can it consistently follow directions and arrive safely at the intended destination, without going off the rails?  
    å®‰å…¨ï¼šå®ƒèƒ½å¦å§‹ç»ˆå¦‚ä¸€åœ°éµå¾ªæŒ‡ç¤ºï¼Œå®‰å…¨åˆ°è¾¾é¢„å®šç›®çš„åœ°ï¼Œè€Œä¸ä¼šå‡ºè½¨ï¼Ÿ
    

Just as youâ€™d never let someone drive without passing their test, you shouldnâ€™t let an AI product launch without passing thoughtful, intentional evals.  
å°±åƒä½ ä¸ä¼šè®©ä¸€ä¸ªæ²¡æœ‰é€šè¿‡è€ƒè¯•çš„äººå¼€è½¦ä¸€æ ·ï¼Œä½ ä¸åº”è¯¥è®©ä¸€ä¸ª AI äº§å“åœ¨æ²¡æœ‰ç»è¿‡æ·±æ€ç†Ÿè™‘ã€æœ‰æ„çš„è¯„ä¼°çš„æƒ…å†µä¸‹å‘å¸ƒã€‚



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164844027.png?cd-oss-obs)


Evals are analogous to unit testing in some ways, with important differences. Traditional software unit testing is like checking if a train stays on its tracks: straightforward, deterministic, clear pass/fail scenarios. Evals for LLM-based systems, on the other hand, can feel more like driving a car through a busy city. The environment is variable, and the system is non-deterministic. Unlike in traditional software testing, when you give the same prompt to an LLM multiple times, you might see slightly different responsesâ€”just like how drivers can behave differently in city traffic. With evals, youâ€™re often dealing with more qualitative or open-ended metricsâ€”like the relevance or coherence of the outputâ€”that might not fit neatly into a strict pass/fail testing model.  
è¯„ä¼°åœ¨æŸäº›æ–¹é¢ç±»ä¼¼äºå•å…ƒæµ‹è¯•ï¼Œä½†ä¹Ÿæœ‰é‡è¦åŒºåˆ«ã€‚ä¼ ç»Ÿçš„è½¯ä»¶å•å…ƒæµ‹è¯•å°±åƒæ£€æŸ¥ç«è½¦æ˜¯å¦åœ¨è½¨é“ä¸Šè¡Œé©¶ï¼šç®€å•ç›´æ¥ã€ç¡®å®šæ€§å¼ºã€æœ‰æ˜ç¡®çš„é€šè¿‡/å¤±è´¥åœºæ™¯ã€‚è€ŒåŸºäºLLMçš„ç³»ç»Ÿçš„è¯„ä¼°ï¼Œå¦ä¸€æ–¹é¢å¯èƒ½æ›´åƒæ˜¯åœ¨ç¹å¿™çš„åŸå¸‚ä¸­é©¾é©¶æ±½è½¦ã€‚ç¯å¢ƒæ˜¯å˜åŒ–çš„ï¼Œç³»ç»Ÿæ˜¯éç¡®å®šæ€§çš„ã€‚ä¸ä¼ ç»Ÿçš„è½¯ä»¶æµ‹è¯•ä¸åŒï¼Œå½“ä½ å¤šæ¬¡ç»™å‡ºç›¸åŒçš„æç¤ºæ—¶ï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°ç•¥æœ‰ä¸åŒçš„å“åº”â€”â€”å°±åƒé©¾é©¶å‘˜åœ¨åŸå¸‚äº¤é€šä¸­å¯èƒ½ä¼šæœ‰ä¸åŒçš„è¡Œä¸ºã€‚åœ¨è¯„ä¼°ä¸­ï¼Œä½ é€šå¸¸è¦å¤„ç†æ›´å¤šå®šæ€§æˆ–å¼€æ”¾å¼æŒ‡æ ‡â€”â€”å¦‚è¾“å‡ºçš„ç›¸å…³æ€§æˆ–è¿è´¯æ€§â€”â€”è¿™äº›å¯èƒ½æ— æ³•å®Œç¾åœ°é€‚åº”ä¸¥æ ¼çš„é€šè¿‡/å¤±è´¥æµ‹è¯•æ¨¡å‹ã€‚



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164845422.jpeg?cd-oss-obs)



An example eval prompt to detect frustrated users  
ä¸€ä¸ªç”¨äºæ£€æµ‹æ²®ä¸§ç”¨æˆ·çš„è¯„ä¼°æç¤ºç¤ºä¾‹

1.  **Human evals:** These are human feedback loops you can design into your product (i.e. showing a thumbs-up/thumbs-down or a comment box next to an LLM response, for your user to provide feedback). You can also have human labelers (i.e. subject-matter experts) provide their labels and feedback, and use this for aligning the application with human preferences via prompt optimization or fine-tuning a model (aka [reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback), or RLHF).  
    äººå·¥è¯„ä¼°ï¼šè¿™äº›æ˜¯å¯ä»¥è®¾è®¡åˆ°æ‚¨äº§å“ä¸­çš„äººä¸ºåé¦ˆå¾ªç¯ï¼ˆä¾‹å¦‚ï¼Œåœ¨LLMå›å¤æ—è¾¹æ˜¾ç¤ºç‚¹èµ/è¸©æˆ–è¯„è®ºæ¡†ï¼Œä»¥ä¾¿ç”¨æˆ·æä¾›åé¦ˆï¼‰ã€‚æ‚¨è¿˜å¯ä»¥è®©äººå·¥æ ‡æ³¨å‘˜ï¼ˆå³é¢†åŸŸä¸“å®¶ï¼‰æä¾›ä»–ä»¬çš„æ ‡ç­¾å’Œåé¦ˆï¼Œå¹¶ä½¿ç”¨è¿™äº›ä¿¡æ¯é€šè¿‡æç¤ºä¼˜åŒ–æˆ–å¾®è°ƒæ¨¡å‹ï¼ˆå³ä»äººä¸ºåé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼Œæˆ– RLHFï¼‰æ¥ä½¿åº”ç”¨ç¨‹åºä¸äººç±»åå¥½ä¿æŒä¸€è‡´ã€‚
    
    *   **Pro:** Directly tied to the end user.  
        ä¼˜ç‚¹ï¼šç›´æ¥ä¸æœ€ç»ˆç”¨æˆ·æŒ‚é’©ã€‚
        
    *   **Cons:** Very sparse (most people donâ€™t hit that thumbs-up/thumbs-down), not a strong signal (what does a thumbs-up or -down mean?), and costly (if you want to hire human labelers).  
        ç¼ºç‚¹ï¼šéå¸¸ç¨€ç–ï¼ˆå¤§å¤šæ•°äººä¸ä¼šç‚¹èµ/ç‚¹è¸©ï¼‰ï¼Œä¸æ˜¯ä¸€ä¸ªå¼ºçƒˆçš„ä¿¡å·ï¼ˆç‚¹èµæˆ–ç‚¹è¸©æ„å‘³ç€ä»€ä¹ˆï¼Ÿï¼‰ï¼Œè€Œä¸”æˆæœ¬é«˜æ˜‚ï¼ˆå¦‚æœä½ æƒ³é›‡ä½£äººå·¥æ ‡æ³¨å‘˜ï¼‰ã€‚
        
2.  **Code-based evals:** Utilizing checks on API calls or code generation (i.e. was the generated code â€œvalidâ€ and can it run?).  
    åŸºäºä»£ç çš„è¯„ä¼°ï¼šåˆ©ç”¨å¯¹ API è°ƒç”¨æˆ–ä»£ç ç”Ÿæˆçš„æ£€æŸ¥ï¼ˆä¾‹å¦‚ï¼Œç”Ÿæˆçš„ä»£ç â€œæœ‰æ•ˆâ€ä¸”èƒ½å¦è¿è¡Œï¼Ÿï¼‰ã€‚
    
    *   **Pros:** Cheap and fast to write this eval.  
        ä¼˜ç‚¹ï¼šç¼–å†™è¿™ä»½è¯„ä¼°æ—¢ä¾¿å®œåˆå¿«ã€‚
        
    *   **Cons:** Not a strong signal; great for code-based LLM generation but not for more nuanced responses or evaluations.  
        ç¼ºç‚¹ï¼šä¿¡å·ä¸å¼ºï¼›éå¸¸é€‚åˆåŸºäºä»£ç çš„LLMç”Ÿæˆï¼Œä½†ä¸é€‚ç”¨äºæ›´ç»†è…»çš„å›åº”æˆ–è¯„ä¼°ã€‚
        
3.  **LLM-based evals:** This technique utilizes an external LLM system (i.e. a â€œjudgeâ€ LLM), with a prompt like the one above, to grade the output of the agent system. LLM-based evals allow you to generate classification labels in an automated way that resembles human-labeled dataâ€”without needing to have users or subject-matter experts label all of your data.  
    åŸºäºLLMçš„è¯„ä¼°ï¼šè¿™ç§æŠ€æœ¯åˆ©ç”¨å¤–éƒ¨LLMç³»ç»Ÿï¼ˆå³â€œè¯„å§”â€LLMï¼‰ï¼Œä½¿ç”¨ä¸Šè¿°ç±»ä¼¼çš„æç¤ºï¼Œæ¥è¯„ä¼°ä»£ç†ç³»ç»Ÿçš„è¾“å‡ºã€‚åŸºäºLLMçš„è¯„ä¼°å…è®¸æ‚¨ä»¥ç±»ä¼¼äºäººå·¥æ ‡æ³¨æ•°æ®çš„æ–¹å¼è‡ªåŠ¨ç”Ÿæˆåˆ†ç±»æ ‡ç­¾â€”â€”æ— éœ€è®©ç”¨æˆ·æˆ–é¢†åŸŸä¸“å®¶å¯¹æ‚¨æ‰€æœ‰æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚
    
    *   **Pro:** Scalable (itâ€™s like a human label but much cheaper) and natural language, so the PM can write prompts. You can also get the LLM to generate an explanation.  
        ä¼˜ç‚¹ï¼šå¯æ‰©å±•ï¼ˆå°±åƒäººå·¥æ ‡ç­¾ä½†æˆæœ¬æ›´ä½ï¼‰ä¸”æ”¯æŒè‡ªç„¶è¯­è¨€ï¼Œå› æ­¤é¡¹ç›®ç»ç†å¯ä»¥ç¼–å†™æç¤ºã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨LLMç”Ÿæˆè§£é‡Šã€‚
        
    *   **Con:** Need to create LLM-as-a-judge (with some small amount of data to start).  
        ç¼ºç‚¹ï¼šéœ€è¦åˆ›å»ºLLM-as-a-judgeï¼ˆåˆå§‹æ—¶éœ€è¦ä¸€äº›å°‘é‡æ•°æ®ï¼‰ã€‚
        

Importantly, LLM-based evals are natural language prompts themselves. That means that just as building intuition for your AI agent or LLM-based system requires prompting, evaluating that same system also requires you to describe what you want to catch.  
é‡è¦çš„æ˜¯ï¼ŒåŸºäºLLMçš„è¯„ä¼°æœ¬èº«å°±æ˜¯è‡ªç„¶è¯­è¨€æç¤ºã€‚è¿™æ„å‘³ç€ï¼Œå°±åƒå»ºç«‹å¯¹æ‚¨çš„ AI ä»£ç†æˆ–LLM-åŸºäºçš„ç³»ç»Ÿè¿›è¡Œç›´è§‰éœ€è¦æç¤ºä¸€æ ·ï¼Œè¯„ä¼°è¯¥ç³»ç»Ÿä¹Ÿéœ€è¦æ‚¨æè¿°æ‚¨æƒ³è¦æ•æ‰çš„å†…å®¹ã€‚

Letâ€™s take the example from earlier: a trip-planning agent. In that system, there are a lot of things that can go wrong, and you can choose the right eval approach for each step in the system.  
è®©æˆ‘ä»¬ä»¥å‰é¢çš„ä¾‹å­ä¸ºä¾‹ï¼šä¸€ä¸ªè¡Œç¨‹è§„åˆ’ä»£ç†ã€‚åœ¨è¯¥ç³»ç»Ÿä¸­ï¼Œæœ‰å¾ˆå¤šäº‹æƒ…å¯èƒ½ä¼šå‡ºé”™ï¼Œå¹¶ä¸”ä½ å¯ä»¥ä¸ºç³»ç»Ÿä¸­çš„æ¯ä¸ªæ­¥éª¤é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•ã€‚



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164847027.jpeg?cd-oss-obs)


As a user, you want evals that are (1) specific, (2) battle-tested, and (3) test for specific areas of success. A few examples of common areas evals might look at are:  
ä½œä¸ºç”¨æˆ·ï¼Œæ‚¨å¸Œæœ›è¯„ä¼°ç»“æœæ˜¯ï¼ˆ1ï¼‰å…·ä½“çš„ï¼Œï¼ˆ2ï¼‰ç»è¿‡å®æˆ˜æ£€éªŒçš„ï¼Œä»¥åŠï¼ˆ3ï¼‰é’ˆå¯¹ç‰¹å®šæˆåŠŸé¢†åŸŸçš„ã€‚è¯„ä¼°å¯èƒ½å…³æ³¨çš„å¸¸è§é¢†åŸŸåŒ…æ‹¬ï¼š

1.  **[Hallucination](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/hallucinations):** Is the agent accurately using the provided context, or is it making things up?  
    å¹»è§‰ï¼šä»£ç†æ˜¯å¦å‡†ç¡®ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œè¿˜æ˜¯å®ƒåœ¨ç¼–é€ ä¸œè¥¿ï¼Ÿ
    
    1.  Useful for: When you are providing documents (e.g. PDFs) for the agent to perform reasoning on top of  
        é€‚ç”¨äºï¼šå½“æ‚¨ä¸ºä»£ç†æä¾›æ–‡æ¡£ï¼ˆä¾‹å¦‚ PDF æ–‡ä»¶ï¼‰ä»¥è¿›è¡Œæ¨ç†æ—¶
        

2.  **[Toxicity/tone](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/toxicity):** Is the agent outputting harmful or undesirable language?  
    æ¯’æ€§/è¯­æ°”ï¼šä»£ç†æ˜¯å¦è¾“å‡ºæœ‰å®³æˆ–ä¸å¸Œæœ›çš„è¯­è¨€ï¼Ÿ
    
    1.  Useful for: End-user applications, to determine if users may be trying to exploit the system or the LLM is responding inappropriately  
        é€‚ç”¨äºï¼šç»ˆç«¯ç”¨æˆ·åº”ç”¨ï¼Œä»¥ç¡®å®šç”¨æˆ·æ˜¯å¦è¯•å›¾åˆ©ç”¨ç³»ç»Ÿæˆ–LLMæ˜¯å¦å“åº”ä¸å½“
        

3.  **[Overall correctness](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/q-and-a-on-retrieved-data):** How well is the system performing at its primary goal?  
    æ€»ä½“æ­£ç¡®æ€§ï¼šç³»ç»Ÿåœ¨å…¶ä¸»è¦ç›®æ ‡ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ
    
    1.  Useful for: End-to-end effectiveness; for example, question-answering accuracyâ€”how often is the agent actually correct at answering a question provided by a user?  
        é€‚ç”¨äºï¼šç«¯åˆ°ç«¯æœ‰æ•ˆæ€§ï¼›ä¾‹å¦‚ï¼Œé—®ç­”å‡†ç¡®æ€§â€”â€”ä»£ç†äººåœ¨å›ç­”ç”¨æˆ·æå‡ºçš„é—®é¢˜æ—¶å®é™…ä¸Šæœ‰å¤šæ­£ç¡®ï¼Ÿ
        

Other common areas for eval would be:  
å…¶ä»–å¸¸è§çš„è¯„ä¼°é¢†åŸŸåŒ…æ‹¬ï¼š

*   [Code generation ä»£ç ç”Ÿæˆ](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/code-generation-eval)
    
*   [Summarization quality æ‘˜è¦è´¨é‡](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/summarization-eval)
    
*   [Retrieval relevance æ£€ç´¢ç›¸å…³æ€§](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/retrieval-rag-relevance)
    

Phoenix (open source) maintains a repository of off-the-shelf evaluators [here](https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals).* Ragas (open source) also maintains a repository of RAG-specific evaluators [here](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/).  
å‡¤å‡°ï¼ˆå¼€æºï¼‰åœ¨æ­¤å¤„ç»´æŠ¤äº†ä¸€ä¸ªç°æˆè¯„ä¼°å™¨çš„å­˜å‚¨åº“ã€‚* æ‹‰åŠ ï¼ˆå¼€æºï¼‰ä¹Ÿåœ¨æ­¤å¤„ç»´æŠ¤äº†ä¸€ä¸ªé’ˆå¯¹ RAG çš„ç‰¹å®šè¯„ä¼°å™¨çš„å­˜å‚¨åº“ã€‚

_*Full disclosure: Iâ€™m a contributor to Phoenix, which is open source (there are other tools out there too for evals, like Ragas). Iâ€™d recommend people get started with something free/open source, which wonâ€™t hold their data hostage, to run evals! Many of the tools in the space are closed source. You never have to talk to Arize/our team to use Phoenix for evals.  
**å®Œå…¨æŠ«éœ²ï¼šæˆ‘æ˜¯ Phoenix çš„è´¡çŒ®è€…ï¼Œå®ƒæ˜¯å¼€æºçš„ï¼ˆè¿˜æœ‰å…¶ä»–ç”¨äºè¯„ä¼°çš„å·¥å…·ï¼Œå¦‚ Ragasï¼‰ã€‚æˆ‘å»ºè®®äººä»¬ä»å…è´¹/å¼€æºçš„è½¯ä»¶å¼€å§‹ï¼Œè¿™æ ·å°±ä¸ä¼šè®©ä»–ä»¬çš„æ•°æ®æˆä¸ºäººè´¨ï¼Œä»¥ä¾¿è¿›è¡Œè¯„ä¼°ï¼è¯¥é¢†åŸŸä¸­çš„è®¸å¤šå·¥å…·éƒ½æ˜¯é—­æºçš„ã€‚æ‚¨ä¸å¿…ä¸ Arize/æˆ‘ä»¬çš„å›¢é˜Ÿäº¤è°ˆå³å¯ä½¿ç”¨ Phoenix è¿›è¡Œè¯„ä¼°ã€‚**_

Each great LLM eval contains four distinct parts:  
æ¯ä¸ªä¼˜ç§€çš„LLMè¯„ä¼°åŒ…å«å››ä¸ªä¸åŒçš„éƒ¨åˆ†ï¼š

*   **Part 1:** **Setting the role.** You need to provide the judge-LLM a role (e.g. â€œyou are examining written textâ€) so that the system is primed for the task.  
    ç¬¬ä¸€éƒ¨åˆ†ï¼šè®¾å®šè§’è‰²ã€‚æ‚¨éœ€è¦ä¸ºæ³•å®˜LLMåˆ†é…ä¸€ä¸ªè§’è‰²ï¼ˆä¾‹å¦‚ï¼šâ€œä½ æ­£åœ¨æ£€æŸ¥ä¹¦é¢æ–‡æœ¬â€ï¼‰ï¼Œä»¥ä¾¿ç³»ç»Ÿä¸ºä»»åŠ¡åšå¥½å‡†å¤‡ã€‚
    
*   **Part 2: Providing the context.** This is the data you will actually be sending to the LLM to grade. This will come from your application (i.e. the message chain, or the message generated from the agent LLM).  
    ç¬¬äºŒéƒ¨åˆ†ï¼šæä¾›èƒŒæ™¯ä¿¡æ¯ã€‚è¿™æ˜¯æ‚¨å°†å®é™…å‘é€åˆ°LLMè¿›è¡Œè¯„åˆ†çš„æ•°æ®ã€‚è¿™äº›æ•°æ®å°†æ¥è‡ªæ‚¨çš„åº”ç”¨ï¼ˆå³æ¶ˆæ¯é“¾ï¼Œæˆ–ç”±ä»£ç†LLMç”Ÿæˆçš„æ¶ˆæ¯ï¼‰ã€‚
    
*   **Part 3: Providing the goal.** Clearly articulating what you want your judge-LLM to measure isnâ€™t just a step in the process; itâ€™s the difference between a mediocre AI and one that consistently delights users. Building these writing skills requires practice and attention. You need to clearly define what success and failure look like to the judge-LLM, translating nuanced user expectations into precise criteria your LLM judge can follow. What do you want the judge-LLM to measure? How would you articulate what a â€œgoodâ€ or â€œbadâ€ outcome is?  
    ç¬¬ä¸‰éƒ¨åˆ†ï¼šè®¾å®šç›®æ ‡ã€‚æ˜ç¡®åœ°é˜è¿°ä½ å¸Œæœ›ä½ çš„è¯„åˆ¤è€…LLMè¦è¡¡é‡çš„å†…å®¹ï¼Œè¿™ä¸ä»…æ˜¯ä¸€ä¸ªæ­¥éª¤ï¼Œè€Œä¸”æ˜¯å¹³åº¸çš„äººå·¥æ™ºèƒ½å’Œèƒ½å¤ŸæŒç»­è®©ç”¨æˆ·æ„Ÿåˆ°æ„‰æ‚¦çš„äººå·¥æ™ºèƒ½ä¹‹é—´çš„åŒºåˆ«ã€‚åŸ¹å…»è¿™äº›å†™ä½œæŠ€èƒ½éœ€è¦ç»ƒä¹ å’Œå…³æ³¨ã€‚ä½ éœ€è¦æ¸…æ¥šåœ°å®šä¹‰å¯¹è¯„åˆ¤è€…LLMæ¥è¯´æˆåŠŸå’Œå¤±è´¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œå°†ç»†å¾®çš„ç”¨æˆ·æœŸæœ›è½¬åŒ–ä¸ºä½ çš„LLMè¯„åˆ¤è€…å¯ä»¥éµå¾ªçš„ç²¾ç¡®æ ‡å‡†ã€‚ä½ å¸Œæœ›è¯„åˆ¤è€…LLMè¡¡é‡ä»€ä¹ˆï¼Ÿä½ å°†å¦‚ä½•é˜è¿°â€œå¥½â€æˆ–â€œåâ€çš„ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ
    
*   **Part 4: Defining the terminology and label.** Toxicity, for example, can mean different things in different contexts. You want to be specific here so the judge-LLM is â€œgroundedâ€ in the terminology you care about.  
    ç¬¬å››éƒ¨åˆ†ï¼šå®šä¹‰æœ¯è¯­å’Œæ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œâ€œæ¯’æ€§â€åœ¨ä¸åŒçš„è¯­å¢ƒä¸­å¯èƒ½æœ‰ä¸åŒçš„å«ä¹‰ã€‚ä½ åœ¨è¿™é‡Œéœ€è¦å…·ä½“æ˜ç¡®ï¼Œä»¥ä¾¿ä½¿æ³•å®˜LLMå¯¹æ‰€å…³å¿ƒçš„æœ¯è¯­â€œæ‰æ ¹â€ã€‚
    

Hereâ€™s a concrete example. Below is an example eval for toxicity/tone for your trip planner agent.  
è¿™é‡Œæœ‰ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ã€‚ä»¥ä¸‹æ˜¯ä¸ºæ‚¨çš„è¡Œç¨‹è§„åˆ’ä»£ç†æä¾›çš„å…³äºæ¯’æ€§/è¯­è°ƒçš„è¯„ä¼°ç¤ºä¾‹ã€‚



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164848369.jpeg?cd-oss-obs)



Evals arenâ€™t just a one-time check. Gathering data to evaluate, writing evals, analyzing the results, and integrating feedback from evals is an iterative workflow from initial development through continuous improvement after launch. Letâ€™s use the trip planning agent example from earlier to illustrate the process for building an eval from scratch.  
è¯„ä¼°ä¸ä»…ä»…æ˜¯å•æ¬¡æ£€æŸ¥ã€‚æ”¶é›†è¯„ä¼°æ•°æ®ã€æ’°å†™è¯„ä¼°æŠ¥å‘Šã€åˆ†æç»“æœä»¥åŠæ•´åˆè¯„ä¼°åé¦ˆæ˜¯ä¸€ä¸ªä»åˆå§‹å¼€å‘åˆ°å‘å¸ƒåæŒç»­æ”¹è¿›çš„è¿­ä»£å·¥ä½œæµç¨‹ã€‚è®©æˆ‘ä»¬ä»¥å‰é¢æåˆ°çš„è¡Œç¨‹è§„åˆ’ä»£ç†ä¸ºä¾‹ï¼Œæ¥è¯´æ˜ä»å¤´å¼€å§‹æ„å»ºè¯„ä¼°çš„è¿‡ç¨‹ã€‚

Letâ€™s say youâ€™ve launched your trip planning agent and are getting feedback from users. Hereâ€™s how you can use that feedback to build out a dataset for evaluation:  
å‡è®¾ä½ å·²ç»å‘å¸ƒäº†ä½ çš„æ—…è¡Œè§„åˆ’ä»£ç†ï¼Œå¹¶ä»ç”¨æˆ·é‚£é‡Œè·å¾—äº†åé¦ˆã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åˆ©ç”¨è¿™äº›åé¦ˆæ¥æ„å»ºç”¨äºè¯„ä¼°çš„æ•°æ®é›†çš„æ–¹æ³•ï¼š

1.  **Gather real user interactions:** [Capture real examples](https://hamel.dev/blog/posts/field-guide/index.html#measure-alignment-between-automated-evals-and-human-judgment) of how users engage with your app. You can do this via direct feedback, analytics, or manual inspection of interactions within your application.  
    æ”¶é›†çœŸå®ç”¨æˆ·äº’åŠ¨ï¼šæ•æ‰ç”¨æˆ·å¦‚ä½•ä¸æ‚¨çš„åº”ç”¨äº’åŠ¨çš„çœŸå®ç¤ºä¾‹ã€‚æ‚¨å¯ä»¥é€šè¿‡ç›´æ¥åé¦ˆã€åˆ†ææˆ–æ‰‹åŠ¨æ£€æŸ¥åº”ç”¨å†…çš„äº’åŠ¨æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
    
    1.  For example: Capture human feedback (thumbs-up/down) from your users interacting with the agent. Try to build out a dataset representative of real-world examples that have human feedback.  
        ä¾‹å¦‚ï¼šæ•æ‰ç”¨æˆ·ä¸ä»£ç†äº’åŠ¨æ—¶çš„äººç±»åé¦ˆï¼ˆç‚¹èµ/è¸©ï¼‰ã€‚å°è¯•æ„å»ºä¸€ä¸ªå…·æœ‰çœŸå®ä¸–ç•Œç¤ºä¾‹çš„äººç±»åé¦ˆçš„æ•°æ®é›†ã€‚
        
    2.  If you donâ€™t collect feedback from your application, you can also take a sample of data and have subject-matter experts (or even PMs!) label the data.  
        å¦‚æœæ‚¨ä¸æ”¶é›†åº”ç”¨ç¨‹åºçš„åé¦ˆï¼Œæ‚¨ä¹Ÿå¯ä»¥æŠ½å–æ•°æ®æ ·æœ¬ï¼Œå¹¶è®©é¢†åŸŸä¸“å®¶ï¼ˆç”šè‡³äº§å“ç»ç†ï¼ï¼‰å¯¹æ•°æ®è¿›è¡Œæ ‡æ³¨ã€‚
        

2.  **Document edge cases:** Identify the unusual or unexpected ways users interact with your AI, as well as any atypical responses from the agent.  
    æ–‡æ¡£è¾¹ç¼˜æƒ…å†µï¼šè¯†åˆ«ç”¨æˆ·ä¸æ‚¨çš„ AI äº’åŠ¨çš„ä¸å¯»å¸¸æˆ–æ„å¤–æ–¹å¼ï¼Œä»¥åŠä»£ç†çš„ä»»ä½•éå…¸å‹å“åº”ã€‚
    
    1.  As you inspect specific examples, you might want a dataset that is balanced across topics. For example:  
        åœ¨æ£€æŸ¥å…·ä½“ç¤ºä¾‹æ—¶ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¸€ä¸ªåœ¨å„ä¸ªä¸»é¢˜ä¸Šå¹³è¡¡çš„æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼š
        
        *   Help booking a hotel å¸®åŠ©é¢„è®¢é…’åº—
            
        *   Help booking a flight å¸®åŠ©é¢„è®¢èˆªç­
            
        *   Asking for support è¯·æ±‚æ”¯æŒ
            
        *   Asking for trip planning advice  
            è¯·æ±‚è¡Œç¨‹è§„åˆ’å»ºè®®
            

3.  **Build a representative dataset:** Collect these interactions into a structured dataset, ideally annotated with â€œground truthâ€ (human labels) for accuracy. Iâ€™d recommend having between 10 and 100 examples with human labels to start with, as a rule of thumb, to use as ground truth for evaluation. Start simpleâ€”spreadsheets are great initiallyâ€”but eventually consider open source tools like [Phoenix](https://phoenix.arize.com/) for logging and managing data efficiently. Iâ€™m biasedâ€”I helped build Phoenix, but only because I was struggling with this myself. My recommendation would be to use a tool that is open source and easy to use for logging your LLM application data and prompts when getting started.  
    æ„å»ºä»£è¡¨æ€§æ•°æ®é›†ï¼šå°†è¿™äº›äº¤äº’æ”¶é›†åˆ°ä¸€ä¸ªç»“æ„åŒ–æ•°æ®é›†ä¸­ï¼Œç†æƒ³æƒ…å†µä¸‹ç”¨â€œçœŸå®æ ‡ç­¾â€ï¼ˆäººå·¥æ ‡ç­¾ï¼‰è¿›è¡Œæ ‡æ³¨ä»¥æé«˜å‡†ç¡®æ€§ã€‚æˆ‘å»ºè®®ä¸€å¼€å§‹ä½¿ç”¨ 10 åˆ° 100 ä¸ªå¸¦æœ‰çœŸå®æ ‡ç­¾çš„ç¤ºä¾‹ä½œä¸ºè¯„ä¼°çš„çœŸå®æ ‡ç­¾ï¼Œè¿™æ˜¯ä¸€ä¸ªç»éªŒæ³•åˆ™ã€‚ä»ç®€å•å¼€å§‹â€”â€”ç”µå­è¡¨æ ¼æœ€åˆå¾ˆæ£’â€”â€”ä½†æœ€ç»ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨åƒ Phoenix è¿™æ ·çš„å¼€æºå·¥å…·æ¥é«˜æ•ˆåœ°è®°å½•å’Œç®¡ç†æ•°æ®ã€‚æˆ‘æœ‰åè§â€”â€”æˆ‘å¸®åŠ©æ„å»ºäº† Phoenixï¼Œä½†è¿™ä»…ä»…æ˜¯å› ä¸ºæˆ‘è‡ªå·±åœ¨å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚æˆ‘çš„å»ºè®®æ˜¯ä½¿ç”¨ä¸€ä¸ªå¼€æºä¸”æ˜“äºä½¿ç”¨çš„å·¥å…·æ¥è®°å½•æ‚¨çš„LLMåº”ç”¨ç¨‹åºæ•°æ®ï¼Œå¹¶åœ¨å¼€å§‹æ—¶æä¾›æç¤ºã€‚
    

Now that you have a dataset consisting of real-world examples, you can start writing an eval to measure a specific metric, and test the eval against the dataset.  
ç°åœ¨æ‚¨å·²ç»æ‹¥æœ‰äº†ä¸€ä¸ªåŒ…å«çœŸå®ä¸–ç•Œç¤ºä¾‹çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥å¼€å§‹ç¼–å†™ä¸€ä¸ªè¯„ä¼°æ¥è¡¡é‡ç‰¹å®šæŒ‡æ ‡ï¼Œå¹¶å°†è¯„ä¼°ä¸æ•°æ®é›†è¿›è¡Œæµ‹è¯•ã€‚

For example: You might be trying to see if your agent ever answers in a tone that reads as unfriendly to the end user. Even if a user of your platform gives negative feedback, you may want your agent to respond in a friendly tone.  
ä¾‹å¦‚ï¼šä½ å¯èƒ½æƒ³çœ‹çœ‹ä½ çš„ä»£ç†æ˜¯å¦æ›¾ä»¥å¯¹ç”¨æˆ·ä¸å‹å¥½çš„è¯­æ°”å›ç­”ã€‚å³ä½¿ä½ çš„å¹³å°ç”¨æˆ·ç»™å‡ºè´Ÿé¢åé¦ˆï¼Œä½ å¯èƒ½ä¹Ÿå¸Œæœ›ä½ çš„ä»£ç†ä»¥å‹å¥½çš„è¯­æ°”å›åº”ã€‚

1.  **Write initial eval prompts:** Clearly specify the scenarios youâ€™re testing for, following the four-part formula above.  
    ç¼–å†™åˆå§‹è¯„ä¼°æç¤ºï¼šæ˜ç¡®æŒ‡å®šä½ æ­£åœ¨æµ‹è¯•çš„åœºæ™¯ï¼Œéµå¾ªä¸Šè¿°å››éƒ¨åˆ†å…¬å¼ã€‚
    
    1.  For example, the initial eval might look something like:  
        ä¾‹å¦‚ï¼Œåˆæ­¥è¯„ä¼°å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
        
        *   **Setting the role:** â€œYou are a judge, evaluating written text.â€  
            è®¾ç½®è§’è‰²ï¼šâ€œä½ æ˜¯ä¸€ä½è¯„åˆ¤å‘˜ï¼Œæ­£åœ¨è¯„ä¼°ä¹¦é¢æ–‡æœ¬ã€‚â€
            
        *   **Providing the context:** â€œHere is the text : {text}â€ â†’ In this case, {text} is a variable, where you will be providing the â€œLLM agent answerâ€ in the [variable of the prompt](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-templates-and-variables).  
            æä¾›ä¸Šä¸‹æ–‡ï¼šâ€œè¿™é‡Œæ˜¯å¯¹åº”çš„æ–‡æœ¬ï¼š{text}â€â†’åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ{text}æ˜¯ä¸€ä¸ªå˜é‡ï¼Œæ‚¨å°†åœ¨æç¤ºçš„å˜é‡ä¸­æä¾›â€œLLM ä»£ç†å›ç­”â€ã€‚
            
        *   **Providing the goal:** â€œDetermine whether the LLM agent response was friendly.â€  
            æä¾›ç›®æ ‡ï¼šâ€œç¡®å®šLLMä»£ç†å“åº”æ˜¯å¦å‹å¥½ã€‚â€
            
        *   **Defining the terminology and label:** â€œâ€˜Friendlyâ€™ would be defined as using an exclamation point in response and generally being helpful. The response should never have a negative tone.â€  
            å®šä¹‰æœ¯è¯­å’Œæ ‡ç­¾ï¼šâ€œâ€˜å‹å¥½â€™å®šä¹‰ä¸ºåœ¨å›å¤ä¸­ä½¿ç”¨æ„Ÿå¹å·ï¼Œå¹¶ä¸”é€šå¸¸æœ‰å¸®åŠ©ã€‚å›å¤ä¸åº”å¸¦æœ‰è´Ÿé¢è¯­æ°”ã€‚â€
            

2.  **Run evals against your dataset:** You will run the eval by sending the eval prompt plus LLM agent answer variable to an LLM, and get back a label for each row in your dataset.  
    è¿è¡Œè¯„ä¼°æ‚¨çš„æ•°æ®é›†ï¼šæ‚¨å°†é€šè¿‡å‘é€è¯„ä¼°æç¤ºç¬¦åŠ ä¸ŠLLMä»£ç†å›ç­”å˜é‡åˆ°LLMæ¥è¿è¡Œè¯„ä¼°ï¼Œå¹¶ä¸ºæ‚¨çš„æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œè·å–ä¸€ä¸ªæ ‡ç­¾ã€‚
    
    1.  Aim for at least 90% accuracy compared with your human-labeled ground truth.  
        ä»¥è‡³å°‘ 90%çš„å‡†ç¡®ç‡ä¸æ‚¨çš„äººç±»æ ‡æ³¨çš„åŸºå‡†çœŸå€¼è¿›è¡Œæ¯”è¾ƒã€‚
        

3.  **Identify patterns in failures:** Where does the eval fall short? Iterate on your prompt.  
    è¯†åˆ«æ•…éšœæ¨¡å¼ï¼šè¯„ä¼°å“ªé‡Œåšå¾—ä¸å¤Ÿå¥½ï¼Ÿè¿­ä»£ä½ çš„æç¤ºã€‚
    
    1.  In the example below: The eval disagrees with the human label in the last example. Our prompt above requires an exclamation point for an LLM agent response to be considered friendly. Maybe that requirement is too strict?  
        åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼šè¯„ä¼°ç»“æœä¸æœ€åä¸€ä¸ªä¾‹å­ä¸­çš„äººç±»æ ‡ç­¾ä¸ä¸€è‡´ã€‚æˆ‘ä»¬ä¸Šé¢çš„æç¤ºè¦æ±‚ä½¿ç”¨æ„Ÿå¹å·ï¼Œä»¥ä¾¿å°†LLMä»£ç†çš„å“åº”è§†ä¸ºå‹å¥½ã€‚ä¹Ÿè®¸è¿™ä¸ªè¦æ±‚å¤ªä¸¥æ ¼äº†ï¼Ÿ
        



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164850311.png?cd-oss-obs)



1.  **Refine eval prompts:** Continuously adjust your prompts based on results until performance meets your standards.  
    ä¼˜åŒ–è¯„ä¼°æç¤ºï¼šæ ¹æ®ç»“æœæŒç»­è°ƒæ•´æ‚¨çš„æç¤ºï¼Œç›´åˆ°æ€§èƒ½è¾¾åˆ°æ‚¨çš„æ ‡å‡†ã€‚
    
    1.  Tip: You can add a few examples to your prompt of â€œgoodâ€ and â€œbadâ€ evals to ground the LLM response, as a form of â€œ[few-shot prompting](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/multishot-prompting).â€  
        æç¤ºï¼šæ‚¨å¯ä»¥åœ¨â€œè‰¯å¥½â€å’Œâ€œä¸è‰¯â€è¯„ä¼°çš„æç¤ºä¸­æ·»åŠ å‡ ä¸ªç¤ºä¾‹ï¼Œä»¥ä½¿LLMçš„å›å¤æ›´æœ‰ä¾æ®ï¼Œè¿™å¯ä»¥è§†ä¸ºä¸€ç§â€œå°‘é‡æ ·æœ¬æç¤ºâ€ã€‚
        
2.  **Expand your dataset:** Regularly add new examples and edge cases to test whether your eval prompts can generalize effectively.  
    æ‰©å±•æ‚¨çš„æ•°æ®é›†ï¼šå®šæœŸæ·»åŠ æ–°çš„ç¤ºä¾‹å’Œè¾¹ç¼˜æƒ…å†µï¼Œä»¥æµ‹è¯•æ‚¨çš„è¯„ä¼°æç¤ºæ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–ã€‚
    
3.  **Iterate on your agent prompt:** Evals help you test your product when you make changes to the underlying AI systemâ€”in some ways, they are the final boss when A/B testing prompts for your AI system. For example, when you make a change to an agent (e.g. changing the model from GPT-4o to Claude 3.7 Sonnet), you can rerun the dataset of questions you collected _through your updated agent_ and evaluate the new output (i.e. Claude 3.7) with your eval agent. The goal would be to improve on your initial agent (GPT-4o) eval scores, giving you a benchmark you can use for continual improvement.  
    è¿­ä»£æ‚¨çš„ä»£ç†æç¤ºï¼šè¯„ä¼°å¯ä»¥å¸®åŠ©æ‚¨åœ¨æ›´æ”¹åº•å±‚ AI ç³»ç»Ÿæ—¶æµ‹è¯•æ‚¨çš„äº§å“â€”â€”åœ¨æŸäº›æ–¹é¢ï¼Œå®ƒä»¬æ˜¯æ‚¨ AI ç³»ç»Ÿ A/B æµ‹è¯•æç¤ºçš„æœ€ç»ˆå¤§ Bossã€‚ä¾‹å¦‚ï¼Œå½“æ‚¨æ›´æ”¹ä»£ç†ï¼ˆä¾‹å¦‚ï¼Œå°†æ¨¡å‹ä» GPT-4o æ›´æ”¹ä¸º Claude 3.7 Sonnetï¼‰æ—¶ï¼Œæ‚¨å¯ä»¥é‡æ–°è¿è¡Œæ‚¨é€šè¿‡æ›´æ–°çš„ä»£ç†æ”¶é›†çš„é—®é¢˜æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ‚¨çš„è¯„ä¼°ä»£ç†è¯„ä¼°æ–°çš„è¾“å‡ºï¼ˆå³ Claude 3.7ï¼‰ã€‚ç›®æ ‡å°†æ˜¯æé«˜æ‚¨æœ€åˆä»£ç†ï¼ˆGPT-4oï¼‰çš„è¯„ä¼°åˆ†æ•°ï¼Œä¸ºæ‚¨æä¾›ä¸€ä¸ªå¯ä»¥ç”¨äºæŒç»­æ”¹è¿›çš„åŸºå‡†ã€‚
    



![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164852137.jpeg?cd-oss-obs)





1.  **Continuous evaluation:** Set up evals to run automatically on live user interactions.  
    æŒç»­è¯„ä¼°ï¼šè®¾ç½®è¯„ä¼°ä»¥è‡ªåŠ¨è¿è¡Œåœ¨å®æ—¶ç”¨æˆ·äº¤äº’ä¸Šã€‚
    
    *   For example: You can continuously run the â€œfriendlyâ€ eval on all your incoming requests and agent responses, to get a score over time. This can help you answer questions such as â€œAre your users getting more frustrated over time?â€ or â€œAre the changes we are making to our system impacting how friendly our LLM is?â€  
        ä¾‹å¦‚ï¼šæ‚¨å¯ä»¥å¯¹æ‰€æœ‰ä¼ å…¥è¯·æ±‚å’Œä»£ç†å“åº”æŒç»­è¿è¡Œâ€œå‹å¥½â€è¯„ä¼°ï¼Œä»¥è·å–éšæ—¶é—´å˜åŒ–çš„åˆ†æ•°ã€‚è¿™å¯ä»¥å¸®åŠ©æ‚¨å›ç­”è¯¸å¦‚â€œç”¨æˆ·æ˜¯å¦éšç€æ—¶é—´çš„æ¨ç§»è€Œå˜å¾—æ›´åŠ æ²®ä¸§ï¼Ÿâ€æˆ–â€œæˆ‘ä»¬å¯¹ç³»ç»Ÿæ‰€åšçš„æ›´æ”¹æ˜¯å¦å½±å“äº†æˆ‘ä»¬çš„LLMçš„å‹å¥½åº¦ï¼Ÿâ€ç­‰é—®é¢˜ã€‚
        
2.  **Compare eval results to actual user outcomes:** Look for discrepancies between eval results and real-world performance (i.e. human-labeled ground truth). Use these insights to enhance your eval framework and improve accuracy over time.  
    æ¯”è¾ƒè¯„ä¼°ç»“æœä¸å®é™…ç”¨æˆ·ç»“æœï¼šå¯»æ‰¾è¯„ä¼°ç»“æœä¸ç°å®ä¸–ç•Œæ€§èƒ½ä¹‹é—´çš„å·®å¼‚ï¼ˆå³äººå·¥æ ‡æ³¨çš„åŸºå‡†çœŸå®æƒ…å†µï¼‰ã€‚åˆ©ç”¨è¿™äº›è§è§£æ¥å®Œå–„æ‚¨çš„è¯„ä¼°æ¡†æ¶å¹¶æé«˜å‡†ç¡®æ€§ã€‚
    
3.  **Build actionable eval dashboards:** Evals can help communicate AI metrics to stakeholders across your team, and they can even be tied to business outcomes. They can serve as proxy leading metrics for changes you make to your system.  
    æ„å»ºå¯æ“ä½œçš„è¯„ä¼°ä»ªè¡¨æ¿ï¼šè¯„ä¼°å¯ä»¥å¸®åŠ©æ‚¨å‘å›¢é˜Ÿä¸­çš„åˆ©ç›Šç›¸å…³è€…ä¼ è¾¾ AI æŒ‡æ ‡ï¼Œç”šè‡³å¯ä»¥å°†å®ƒä»¬ä¸ä¸šåŠ¡æˆæœè”ç³»èµ·æ¥ã€‚å®ƒä»¬å¯ä»¥ä½œä¸ºæ‚¨å¯¹ç³»ç»Ÿè¿›è¡Œçš„æ›´æ”¹çš„ä»£ç†é¢†å…ˆæŒ‡æ ‡ã€‚
    



![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8ed7a15e-79ee-492e-a17e-7c1e8092a4d7_1232x720.gif)




_Running your evaluation continuously on production data  
æŒç»­åœ¨ç”Ÿäº§æ•°æ®ä¸Šè¿è¡Œè¯„ä¼°_

1.  Making evals too complex too quickly can create â€œnoisyâ€ signals (and cause the team to lose trust in the approach). Focus on specific outputs rather than complex evaluationsâ€”you can always add sophistication later.  
    ä½¿è¯„ä¼°è¿‡äºå¤æ‚è¿‡å¿«å¯èƒ½ä¼šäº§ç”Ÿâ€œå˜ˆæ‚â€çš„ä¿¡å·ï¼ˆå¹¶å¯¼è‡´å›¢é˜Ÿå¯¹è¿™ç§æ–¹æ³•å¤±å»ä¿¡ä»»ï¼‰ã€‚ä¸“æ³¨äºå…·ä½“çš„è¾“å‡ºï¼Œè€Œä¸æ˜¯å¤æ‚çš„è¯„ä¼°â€”â€”ä½ æ€»æ˜¯å¯ä»¥åæ¥å¢åŠ å¤æ‚æ€§ã€‚
    
2.  Not testing for edge cases. Provide one or two specific examples of â€œgoodâ€ and â€œbadâ€ evals as part of your promptâ€”few-shot promptingâ€”for increased eval performance. This helps ground the judge-LLM in what is considered good or bad.  
    ä¸æµ‹è¯•è¾¹ç¼˜æƒ…å†µã€‚åœ¨æç¤ºä¸­æä¾›ä¸€åˆ°ä¸¤ä¸ªâ€œå¥½â€å’Œâ€œåâ€è¯„ä¼°çš„å…·ä½“ä¾‹å­â€”â€”ä½œä¸ºå°‘æ ·æœ¬æç¤ºï¼Œä»¥æé«˜è¯„ä¼°æ€§èƒ½ã€‚è¿™æœ‰åŠ©äºè®©è¯„åˆ¤è€…LLMäº†è§£ä½•ä¸ºå¥½æˆ–åã€‚
    
3.  Forgetting to validate eval results against real user feedback. Remember that youâ€™re not just testing code, youâ€™re validating if your AI can truly solve user problems.  
    å¿˜è®°å°†è¯„ä¼°ç»“æœä¸çœŸå®ç”¨æˆ·åé¦ˆè¿›è¡ŒéªŒè¯ã€‚è®°ä½ï¼Œä½ ä¸ä»…ä»…æ˜¯æµ‹è¯•ä»£ç ï¼Œè€Œæ˜¯åœ¨éªŒè¯ä½ çš„ AI æ˜¯å¦çœŸæ­£èƒ½å¤Ÿè§£å†³ç”¨æˆ·é—®é¢˜ã€‚
    

Writing good evals forces you into the shoes of your userâ€”they are how you catch â€œbadâ€ scenarios and know what to improve on.  
æ’°å†™è‰¯å¥½çš„è¯„ä¼°è¿«ä½¿ä½ ç«™åœ¨ç”¨æˆ·çš„è§’åº¦â€”â€”å®ƒä»¬æ˜¯ä½ æ•æ‰â€œä¸è‰¯â€åœºæ™¯å¹¶äº†è§£éœ€è¦æ”¹è¿›ä¹‹å¤„çš„æ–¹å¼ã€‚

Now that you understand the fundamentals, hereâ€™s exactly how to start with evals in your own product:  
ç°åœ¨ä½ å·²ç»äº†è§£äº†åŸºç¡€çŸ¥è¯†ï¼Œä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨æ‚¨çš„äº§å“ä¸­å¼€å§‹è¿›è¡Œè¯„ä¼°çš„è¯¦ç»†æ­¥éª¤ï¼š

1.  **Pick one critical feature** of your AI product to evaluate. A common starting point is â€œhallucination detectionâ€ for a chatbot or agent that relies on documents or context you provide it with to answer questions. Try to tackle evaluating a well-defined component in your system before evaluating deeply internal logic.  
    é€‰æ‹©æ‚¨ AI äº§å“çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§è¿›è¡Œè¯„ä¼°ã€‚ä¸€ä¸ªå¸¸è§çš„èµ·ç‚¹æ˜¯ä¸ºä¾èµ–äºæ‚¨æä¾›çš„æ–‡æ¡£æˆ–ä¸Šä¸‹æ–‡æ¥å›ç­”é—®é¢˜çš„èŠå¤©æœºå™¨äººæˆ–ä»£ç†è¿›è¡Œâ€œå¹»è§‰æ£€æµ‹â€ã€‚åœ¨è¯„ä¼°æ·±å±‚å†…éƒ¨é€»è¾‘ä¹‹å‰ï¼Œå°½é‡å…ˆè¯„ä¼°ç³»ç»Ÿä¸­å®šä¹‰è‰¯å¥½çš„ç»„ä»¶ã€‚
    
2.  **Write a simple eval** checking whether the LLM output correctly references provided content or if it invents (hallucinates) information.  
    ç¼–å†™ä¸€ä¸ªç®€å•çš„è¯„ä¼°ï¼Œæ£€æŸ¥LLMè¾“å‡ºæ˜¯å¦æ­£ç¡®å¼•ç”¨äº†æä¾›çš„å†…å®¹ï¼Œæˆ–è€…æ˜¯å¦å‘æ˜ï¼ˆè™šæ„ï¼‰äº†ä¿¡æ¯ã€‚
    
3.  **Run your eval** on 5 to 10 representative examples from real interactions that you have collected or created.  
    è¿è¡Œæ‚¨çš„è¯„ä¼°ï¼ŒåŸºäºæ‚¨æ”¶é›†æˆ–åˆ›å»ºçš„ 5 åˆ° 10 ä¸ªå…·æœ‰ä»£è¡¨æ€§çš„çœŸå®äº’åŠ¨ç¤ºä¾‹ã€‚
    
4.  **Review the results and iterate**, refining the eval prompt until accuracy improves.  
    å®¡æŸ¥ç»“æœå¹¶è¿­ä»£ï¼Œç›´åˆ°å‡†ç¡®æ€§æé«˜ï¼Œä¸æ–­ä¼˜åŒ–è¯„ä¼°æç¤ºã€‚
    

For a detailed example of how to build a hallucination eval, check out our guide [here](https://arize.com/llm-evaluation), as well as our hands-on course [on Evaluating AI Agents](https://www.deeplearning.ai/short-courses/evaluating-ai-agents/).  
å…³äºå¦‚ä½•æ„å»ºå¹»è§‰è¯„ä¼°çš„è¯¦ç»†ç¤ºä¾‹ï¼Œè¯·å‚é˜…æˆ‘ä»¬åœ¨æ­¤å¤„çš„æŒ‡å—ï¼Œä»¥åŠæˆ‘ä»¬å…³äºè¯„ä¼° AI ä»£ç†çš„å®æ“è¯¾ç¨‹ã€‚

As AI products become more complex, the ability to write good evals will become increasingly crucial. Evals are not just about catching bugs; they help ensure that your AI system consistently delivers value and delights your users! Evals are a critical step in going from prototype to production with generative AI.  
éšç€äººå·¥æ™ºèƒ½äº§å“å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç¼–å†™è‰¯å¥½è¯„ä¼°çš„èƒ½åŠ›å°†å˜å¾—è¶Šæ¥è¶Šå…³é”®ã€‚è¯„ä¼°ä¸ä»…ä»…æ˜¯æ•æ‰é”™è¯¯ï¼›å®ƒä»¬æœ‰åŠ©äºç¡®ä¿æ‚¨çš„ AI ç³»ç»ŸæŒç»­æä¾›ä»·å€¼å¹¶è®©ç”¨æˆ·æ„Ÿåˆ°æ»¡æ„ï¼è¯„ä¼°æ˜¯ä½¿ç”¨ç”Ÿæˆå¼ AI ä»åŸå‹åˆ°ç”Ÿäº§çš„å…³é”®æ­¥éª¤ã€‚


![](https://chengdu-obsidian-milkkey.oss-cn-chengdu.aliyuncs.com/img/20250418164855179.png?cd-oss-obs)



I would love to hear from you: How are you currently evaluating your AI products? What challenges have you faced? Leave a commentğŸ‘‡  
æˆ‘å¾ˆä¹æ„å¬å¬æ‚¨çš„æ„è§ï¼šæ‚¨ç›®å‰æ˜¯å¦‚ä½•è¯„ä¼°æ‚¨çš„ AI äº§å“çš„ï¼Ÿæ‚¨é‡åˆ°äº†å“ªäº›æŒ‘æˆ˜ï¼Ÿç•™ä¸‹è¯„è®ºğŸ‘‡

[Leave a comment ç•™ä¸‹è¯„è®º](https://www.lennysnewsletter.com/p/beyond-vibe-checks-a-pms-complete/comments)

1.  [A conversation with OpenAIâ€™s CPO Kevin Weil, Anthropicâ€™s CPO Mike Krieger, and Sarah Guo  
    ä¸ OpenAI é¦–å¸­äº§å“å®˜ Kevin Weilã€Anthropic é¦–å¸­äº§å“å®˜ Mike Krieger å’Œ Sarah Guo çš„å¯¹è¯](https://www.youtube.com/watch?v=IxkvVZua28k&t=1s)
    
2.  Peter Yang + Aman: [The AI Skill That Will Define Your PM Career in 2025 | Aman Khan (Arize)](https://www.youtube.com/watch?v=u8lEDw7pOkE)  
    æ¨å½¼å¾— + Amanï¼šå®šä¹‰æ‚¨ 2025 å¹´äº§å“ç»ç†ç”Ÿæ¶¯çš„ AI æŠ€èƒ½ | Aman Khanï¼ˆArizeï¼‰
    
3.  [DeepLearning.ai course on evals](http://deeplearning.ai/) with Andrew Ng  
    DeepLearning.ai è¯¾ç¨‹ï¼Œç”± Andrew Ng æ•™æˆçš„è¯„ä¼°è¯¾ç¨‹
    
4.  Prompt optimization guide: [https://arize.com/course/prompt-optimization/](https://arize.com/course/prompt-optimization/)  
    æç¤ºä¼˜åŒ–æŒ‡å—ï¼šhttps://arize.com/course/prompt-optimization/
    
5.  Arize eval hub (free!): [https://arize.com/llm-evaluation](https://arize.com/llm-evaluation)  
    Arize è¯„ä¼°ä¸­å¿ƒï¼ˆå…è´¹ï¼ï¼‰ï¼šhttps://arize.com/llm-evaluation
    
6.  Peter Yang + Scott White: [Exclusive: Inside the Best AI Model for Coding and Writing | Scott White (Anthropic)](https://www.youtube.com/watch?v=ynWgKSF0TZY)  
    æ¨å½¼å¾— + æ–¯ç§‘ç‰¹Â·æ€€ç‰¹ï¼šç‹¬å®¶ï¼šæ­ç§˜æœ€ä½³ç¼–ç å’Œå†™ä½œ AI æ¨¡å‹ | æ–¯ç§‘ç‰¹Â·æ€€ç‰¹ï¼ˆAnthropicï¼‰
    

_Thank you, Aman! è°¢è°¢ï¼ŒAmanï¼_

_Have a fulfilling and productive week ğŸ™  
ç¥æ‚¨æœ‰ä¸€ä¸ªå……å®è€Œå¯Œæœ‰æˆæ•ˆçš„ä¸€å‘¨ ğŸ™_

**If youâ€™re finding this newsletter valuable, share it with a friend, and consider subscribing if you havenâ€™t already. There are [group discounts](https://www.lennysnewsletter.com/subscribe?group=true), [gift options](https://www.lennysnewsletter.com/subscribe?gift=true), and [referral bonuses](https://www.lennysnewsletter.com/leaderboard) available.  
å¦‚æœæ‚¨è§‰å¾—è¿™ä»½é€šè®¯æœ‰ä»·å€¼ï¼Œè¯·ä¸æœ‹å‹åˆ†äº«ï¼Œå¹¶è€ƒè™‘å¦‚æœæ‚¨è¿˜æ²¡æœ‰çš„è¯è®¢é˜…ã€‚æœ‰å›¢ä½“æŠ˜æ‰£ã€ç¤¼å“é€‰é¡¹å’Œæ¨èå¥–é‡‘å¯ç”¨ã€‚**

Sincerely, è¯šæŒšåœ°

Lenny ğŸ‘‹